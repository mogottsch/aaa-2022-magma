{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, gc\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_aggregated_df_init = pd.read_parquet(TRIPS_GROUPED_SPATIO_TEMPORAL_PATH)\n",
    "trips_aggregated_df = trips_aggregated_df_init\n",
    "trips_aggregated_df_reduced = trips_aggregated_df_init[\n",
    "    (trips_aggregated_df_init['h3_res'] == ORIGIN_DESTINATION_H3_RESOLUTION) & \n",
    "    (trips_aggregated_df_init['time_interval_length'] == ORIGIN_DESTINATION_TIME_INTERVAL_LENGTH)\n",
    "]\n",
    "trips_aggregated_df_reduced = trips_aggregated_df_reduced.reset_index(drop=True)\n",
    "availability_df = pd.read_parquet(AVAILABILITY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "availability_df = availability_df.reset_index().rename(\n",
    "    columns={\"datetime\": \"datetime_start_floored\", \"hex_id\": \"start_hex_id\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the demand dataset for all h3 resolutions and all time intervals: 4253147\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of entries in the demand dataset for all h3 resolutions and all time intervals: {trips_aggregated_df_init.index.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# trips_aggregated_df['hour_sin'] = np.sin(trips_aggregated_df.hour*(2.*np.pi/24))\n",
    "# trips_aggregated_df['hour_cos'] = np.cos(trips_aggregated_df.hour*(2.*np.pi/24))\n",
    "# # m\n",
    "# trips_aggregated_df['month_sin'] = np.sin((trips_aggregated_df.month-1)*(2.*np.pi/12))\n",
    "# trips_aggregated_df['month_cos'] = np.cos((trips_aggregated_df.month-1)*(2.*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_temperature</th>\n",
       "      <th>max_temperature</th>\n",
       "      <th>mean_temperature</th>\n",
       "      <th>mean_mean_wind_speed</th>\n",
       "      <th>mean_total_cloud_cover</th>\n",
       "      <th>sum_precipitation</th>\n",
       "      <th>time_interval_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MESS_DATUM</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>7.398438</td>\n",
       "      <td>7.398438</td>\n",
       "      <td>7.398438</td>\n",
       "      <td>2.800781</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>7.699219</td>\n",
       "      <td>7.699219</td>\n",
       "      <td>7.699219</td>\n",
       "      <td>2.900391</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     min_temperature  max_temperature  mean_temperature  \\\n",
       "MESS_DATUM                                                                \n",
       "2019-01-01 00:00:00         7.398438         7.398438          7.398438   \n",
       "2019-01-01 01:00:00         7.699219         7.699219          7.699219   \n",
       "\n",
       "                     mean_mean_wind_speed  mean_total_cloud_cover  \\\n",
       "MESS_DATUM                                                          \n",
       "2019-01-01 00:00:00              2.800781                     8.0   \n",
       "2019-01-01 01:00:00              2.900391                     8.0   \n",
       "\n",
       "                     sum_precipitation  time_interval_length  \n",
       "MESS_DATUM                                                    \n",
       "2019-01-01 00:00:00                0.0                     1  \n",
       "2019-01-01 01:00:00                0.0                     1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the weather dataframe, reduce data types\n",
    "weather_df = pd.read_parquet(WEATHER_AGGR_TEMPORAL_PATH)\n",
    "weather_df = weather_df.astype(np.float16)\n",
    "weather_df['time_interval_length'] = weather_df.time_interval_length.astype(np.uint8)\n",
    "weather_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge(\n",
    "        df,\n",
    "        weather_df,\n",
    "        how=\"left\",\n",
    "        left_on=[\"datetime_start_floored\", \"time_interval_length\"],\n",
    "        right_on=[\"MESS_DATUM\", \"time_interval_length\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge weather data with trips data\n",
    "trips_aggregated_df = add_weather_data(trips_aggregated_df)\n",
    "trips_aggregated_df_reduced = add_weather_data(trips_aggregated_df_reduced)\n",
    "availability_df = add_weather_data(availability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del weather_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load poi data and reduce datatypes\n",
    "hexagons_with_pois_df = pd.read_parquet(HEXAGON_WITH_POIS_PATH)\n",
    "hexagons_with_pois_df = hexagons_with_pois_df.drop(columns=[\"hex_and_neighbors\"])\n",
    "hexagons_with_pois_df = hexagons_with_pois_df.astype({\n",
    "    \"h3_res\": np.uint16,\n",
    "    \"sustenance_poi\": np.uint16,\n",
    "    \"public_transport_poi\": np.uint16,\n",
    "    \"education_poi\": np.uint16,\n",
    "    \"arts_and_culture_poi\": np.uint16,\n",
    "    \"sports_poi\": np.uint16,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_poi_data(df: pd.DataFrame, on=\"start\") -> pd.DataFrame:\n",
    "    left_on = f\"{on}_hex_id\"\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        hexagons_with_pois_df.drop(columns=[\"h3_res\"]).add_suffix(f\"_{on}\"),\n",
    "        left_on=left_on,\n",
    "        right_on=f\"hex_{on}\",\n",
    "    )\n",
    "    df = df.drop(columns={f\"hex_{on}\"})\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_aggregated_df = add_poi_data(trips_aggregated_df)\n",
    "trips_aggregated_df_reduced = add_poi_data(trips_aggregated_df_reduced, on=\"start\")\n",
    "trips_aggregated_df_reduced = add_poi_data(trips_aggregated_df_reduced, on=\"end\")\n",
    "availability_df = add_poi_data(availability_df, on=\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime_start_floored', 'start_hex_id', 'end_hex_id', 'demand',\n",
       "       'h3_res', 'time_interval_length', 'min_temperature', 'max_temperature',\n",
       "       'mean_temperature', 'mean_mean_wind_speed', 'mean_total_cloud_cover',\n",
       "       'sum_precipitation', 'sustenance_poi_start',\n",
       "       'public_transport_poi_start', 'education_poi_start',\n",
       "       'arts_and_culture_poi_start', 'sports_poi_start', 'sustenance_poi_end',\n",
       "       'public_transport_poi_end', 'education_poi_end',\n",
       "       'arts_and_culture_poi_end', 'sports_poi_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_aggregated_df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del hexagons_with_pois_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>land_use</th>\n",
       "      <th>land_use_1</th>\n",
       "      <th>land_use_2</th>\n",
       "      <th>land_use_3</th>\n",
       "      <th>land_use_4</th>\n",
       "      <th>land_use_5</th>\n",
       "      <th>land_use_6</th>\n",
       "      <th>land_use_7</th>\n",
       "      <th>land_use_8</th>\n",
       "      <th>land_use_9</th>\n",
       "      <th>land_use_10</th>\n",
       "      <th>...</th>\n",
       "      <th>land_use_13</th>\n",
       "      <th>land_use_14</th>\n",
       "      <th>land_use_15</th>\n",
       "      <th>land_use_16</th>\n",
       "      <th>land_use_17</th>\n",
       "      <th>land_use_18</th>\n",
       "      <th>land_use_19</th>\n",
       "      <th>land_use_20</th>\n",
       "      <th>land_use_21</th>\n",
       "      <th>land_use_22</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hexagon_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>871f1a140ffffff</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871f1a144ffffff</th>\n",
       "      <td>0.148926</td>\n",
       "      <td>0.155518</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.084351</td>\n",
       "      <td>0.135986</td>\n",
       "      <td>0.058533</td>\n",
       "      <td>0.071045</td>\n",
       "      <td>0.082703</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.031219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "land_use         land_use_1  land_use_2  land_use_3  land_use_4  land_use_5  \\\n",
       "hexagon_id                                                                    \n",
       "871f1a140ffffff    0.000000    0.000000    0.000000    0.000000    0.006058   \n",
       "871f1a144ffffff    0.148926    0.155518    0.016571    0.084351    0.135986   \n",
       "\n",
       "land_use         land_use_6  land_use_7  land_use_8  land_use_9  land_use_10  \\\n",
       "hexagon_id                                                                     \n",
       "871f1a140ffffff    0.000000    0.019272    0.015480    0.000000     0.319092   \n",
       "871f1a144ffffff    0.058533    0.071045    0.082703    0.004414     0.031219   \n",
       "\n",
       "land_use         ...  land_use_13  land_use_14  land_use_15  land_use_16  \\\n",
       "hexagon_id       ...                                                       \n",
       "871f1a140ffffff  ...     0.000000          0.0          0.0     0.007874   \n",
       "871f1a144ffffff  ...     0.000406          0.0          0.0     0.000000   \n",
       "\n",
       "land_use         land_use_17  land_use_18  land_use_19  land_use_20  \\\n",
       "hexagon_id                                                            \n",
       "871f1a140ffffff          0.0     0.000000          0.0     0.001923   \n",
       "871f1a144ffffff          0.0     0.004196          0.0     0.070251   \n",
       "\n",
       "land_use         land_use_21  land_use_22  \n",
       "hexagon_id                                 \n",
       "871f1a140ffffff     0.008865          0.0  \n",
       "871f1a144ffffff     0.000000          0.0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load land use data and reduce data types\n",
    "hexagons_with_land_use_df = pd.read_parquet(HEXAGONS_WITH_LAND_USE_PATH)\n",
    "hexagons_with_land_use_df = hexagons_with_land_use_df.astype(np.float16)\n",
    "hexagons_with_land_use_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_land_use_data(df: pd.DataFrame, on=\"start\") -> pd.DataFrame:\n",
    "\tleft_on = f\"{on}_hex_id\"\n",
    "\treturn pd.merge(\n",
    "\t\tdf,\n",
    "\t\thexagons_with_land_use_df.add_suffix(f\"_{on}\"),\n",
    "\t\tleft_on=left_on,\n",
    "\t\tright_on=f\"hexagon_id\",\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add land use data as feature of the start location\n",
    "trips_aggregated_df = add_land_use_data(trips_aggregated_df)\n",
    "trips_aggregated_df_reduced = add_land_use_data(trips_aggregated_df_reduced, on=\"start\")\n",
    "trips_aggregated_df_reduced = add_land_use_data(trips_aggregated_df_reduced, on=\"end\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime_start_floored', 'start_hex_id', 'end_hex_id', 'demand',\n",
       "       'h3_res', 'time_interval_length', 'min_temperature', 'max_temperature',\n",
       "       'mean_temperature', 'mean_mean_wind_speed', 'mean_total_cloud_cover',\n",
       "       'sum_precipitation', 'sustenance_poi_start',\n",
       "       'public_transport_poi_start', 'education_poi_start',\n",
       "       'arts_and_culture_poi_start', 'sports_poi_start', 'sustenance_poi_end',\n",
       "       'public_transport_poi_end', 'education_poi_end',\n",
       "       'arts_and_culture_poi_end', 'sports_poi_end', 'land_use_1_start',\n",
       "       'land_use_2_start', 'land_use_3_start', 'land_use_4_start',\n",
       "       'land_use_5_start', 'land_use_6_start', 'land_use_7_start',\n",
       "       'land_use_8_start', 'land_use_9_start', 'land_use_10_start',\n",
       "       'land_use_11_start', 'land_use_12_start', 'land_use_13_start',\n",
       "       'land_use_14_start', 'land_use_15_start', 'land_use_16_start',\n",
       "       'land_use_17_start', 'land_use_18_start', 'land_use_19_start',\n",
       "       'land_use_20_start', 'land_use_21_start', 'land_use_22_start',\n",
       "       'land_use_1_end', 'land_use_2_end', 'land_use_3_end', 'land_use_4_end',\n",
       "       'land_use_5_end', 'land_use_6_end', 'land_use_7_end', 'land_use_8_end',\n",
       "       'land_use_9_end', 'land_use_10_end', 'land_use_11_end',\n",
       "       'land_use_12_end', 'land_use_13_end', 'land_use_14_end',\n",
       "       'land_use_15_end', 'land_use_16_end', 'land_use_17_end',\n",
       "       'land_use_18_end', 'land_use_19_end', 'land_use_20_end',\n",
       "       'land_use_21_end', 'land_use_22_end'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_aggregated_df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del hexagons_with_land_use_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory usage: 966.1413087844849 MB\n",
      "memory usage: 13.412040710449219 MB\n",
      "memory usage: 1926.7611694335938 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"memory usage: {trips_aggregated_df.memory_usage(index=True, deep=True).sum() / 1024**2} MB\")\n",
    "print(f\"memory usage: {trips_aggregated_df_reduced.memory_usage(index=True, deep=True).sum() / 1024**2} MB\")\n",
    "print(f\"memory usage: {availability_df.memory_usage(index=True, deep=True).sum() / 1024**2} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_id_map = dict(\n",
    "    enumerate(\n",
    "        np.unique(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    trips_aggregated_df.start_hex_id.unique(),\n",
    "                    trips_aggregated_df.end_hex_id.unique(),\n",
    "                    trips_aggregated_df_reduced.start_hex_id.unique(),\n",
    "                    trips_aggregated_df_reduced.end_hex_id.unique(),\n",
    "                    availability_df.start_hex_id.unique(),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "hex_id_map = {v: k for k, v in hex_id_map.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_hex_ids(df: pd.DataFrame, on=\"start\") -> pd.DataFrame:\n",
    "\treturn df[f\"{on}_hex_id\"].map(hex_id_map).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_aggregated_df[\"start_hex_id\"] = remap_hex_ids(trips_aggregated_df)\n",
    "\n",
    "trips_aggregated_df_reduced[\"start_hex_id\"] = remap_hex_ids(trips_aggregated_df_reduced, on=\"start\")\n",
    "trips_aggregated_df_reduced[\"end_hex_id\"] = remap_hex_ids(trips_aggregated_df_reduced, on=\"end\")\n",
    "\n",
    "availability_df[\"start_hex_id\"] = remap_hex_ids(availability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_aggregated_df.to_feather(MODEL_DATA_PATH)\n",
    "trips_aggregated_df_reduced.to_feather(REDUCED_MODEL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features_to_model_data(model_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    # add time features\n",
    "    model_data['hour'] = model_data.datetime_start_floored.dt.hour\n",
    "    model_data['weekday'] = model_data.datetime_start_floored.dt.weekday\n",
    "    model_data['month'] = model_data.datetime_start_floored.dt.month\n",
    "\n",
    "    # convert time features to dummy variables (one-hot encoding) and replace the original features\n",
    "    hour_dummies = pd.get_dummies(model_data['hour']).add_prefix('hour_')\n",
    "    weekday_dummies = pd.get_dummies(model_data['weekday']).add_prefix('weekday_')\n",
    "    month_dummies = pd.get_dummies(model_data['month']).add_prefix('month_')\n",
    "\n",
    "    model_data.drop(['hour', 'weekday', 'month'], axis=1, inplace=True)\n",
    "    model_data = pd.concat([model_data, hour_dummies, weekday_dummies, month_dummies], axis=1)\n",
    "\n",
    "    del hour_dummies, weekday_dummies, month_dummies\n",
    "    gc.collect()\n",
    "    # remove datetime_start_floored column as it won't be needed anymore\n",
    "\n",
    "    model_data = model_data.drop(columns=['datetime_start_floored'])\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invalid_cols(df : pd.DataFrame):\n",
    "    cols = df.sum().apply(lambda x: np.isinf(x))\n",
    "    return cols[cols].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_invalid_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    invalid_cols = get_invalid_cols(df)\n",
    "\n",
    "    # check if sum of all values in each column can be represented by the dtype of the column\n",
    "    # if this is not the case we cannot scale the data as calculated the mean will return erroneous values\n",
    "    if len(invalid_cols) > 0:\n",
    "        for col in invalid_cols:\n",
    "            if df[col].dtype != np.float16:\n",
    "                raise ValueError(\n",
    "                    \"Unexpected dtype for column {}. Expected float16, got {}\".format(\n",
    "                        col, df[col].dtype\n",
    "                    )\n",
    "                )\n",
    "            df[col] = model_data[col].astype(np.float32)\n",
    "\n",
    "    invalid_cols = get_invalid_cols(df)\n",
    "    if len(invalid_cols) > 0:\n",
    "        raise ValueError(\"Invalid columns remaining\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_temperature_features(model_data: pd.DataFrame, t: int) -> pd.DataFrame:\n",
    "    if t > 5:\n",
    "        return model_data.drop(columns=[\"mean_temperature\"])\n",
    "    return model_data.drop(columns=[\"min_temperature\", \"max_temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_DATA_DIR_PATH):\n",
    "    os.makedirs(MODEL_DATA_DIR_PATH)\n",
    "\n",
    "# store model data for each time and hex resolution in one file\n",
    "# additionally create dummies for start and end hexagons\n",
    "for h3_res, t, df_name in itertools.product(\n",
    "    CALC_H3_RESOLUTIONS,\n",
    "    CALC_TIME_INTERVAL_LENGTHS,\n",
    "    [\"trips\", \"availability\"],\n",
    "):\n",
    "    model_data = trips_aggregated_df if df_name == \"demand\" else availability_df\n",
    "\n",
    "    model_data = model_data[\n",
    "        (model_data.h3_res == h3_res)\n",
    "        & (model_data.time_interval_length == t)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    model_data = choose_temperature_features(model_data, t)\n",
    "\n",
    "    model_data = add_time_features_to_model_data(model_data)\n",
    "\n",
    "    start_hex_dummies = pd.get_dummies(model_data.start_hex_id, prefix=\"start_\")\n",
    "    model_data = pd.concat([model_data, start_hex_dummies], axis=1)\n",
    "    model_data = model_data.drop(\n",
    "        columns=[\"start_hex_id\", \"h3_res\", \"time_interval_length\"]\n",
    "    )\n",
    "    if \"end_hex_id\" in model_data.columns:\n",
    "        model_data = model_data.drop(columns=[\"end_hex_id\"])\n",
    "\n",
    "    model_data = fix_invalid_cols(model_data)\n",
    "\n",
    "    model_data.to_feather(\n",
    "        os.path.join(MODEL_DATA_DIR_PATH, f\"{df_name}_{h3_res}_{t}.feather\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/miniconda3/envs/AAA_MAGMA/lib/python3.9/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    }
   ],
   "source": [
    "trips_aggregated_df_reduced = choose_temperature_features(\n",
    "    trips_aggregated_df_reduced, ORIGIN_DESTINATION_TIME_INTERVAL_LENGTH\n",
    ")\n",
    "\n",
    "trips_aggregated_df_reduced = add_time_features_to_model_data(\n",
    "    trips_aggregated_df_reduced\n",
    ")\n",
    "\n",
    "start_hex_dummies = pd.get_dummies(\n",
    "    trips_aggregated_df_reduced.start_hex_id, prefix=\"start_\"\n",
    ")\n",
    "end_hex_dummies = pd.get_dummies(trips_aggregated_df_reduced.end_hex_id, prefix=\"end_\")\n",
    "trips_aggregated_df_reduced = pd.concat(\n",
    "    [trips_aggregated_df_reduced, start_hex_dummies, end_hex_dummies], axis=1\n",
    ")\n",
    "trips_aggregated_df_reduced = trips_aggregated_df_reduced.drop(\n",
    "    columns=[\"start_hex_id\", \"end_hex_id\", \"h3_res\", \"time_interval_length\"]\n",
    ")\n",
    "\n",
    "trips_aggregated_df_reduced = fix_invalid_cols(trips_aggregated_df_reduced)\n",
    "\n",
    "trips_aggregated_df_reduced.to_feather(\n",
    "    os.path.join(\n",
    "        MODEL_DATA_DIR_PATH,\n",
    "        f\"demand_{ORIGIN_DESTINATION_H3_RESOLUTION}_{ORIGIN_DESTINATION_TIME_INTERVAL_LENGTH}.feather\",\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('AAA_MAGMA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f128d9a36ac5782f4755de02247e2ed06e0bf1d935493c1f1cb8e21863a0d39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
