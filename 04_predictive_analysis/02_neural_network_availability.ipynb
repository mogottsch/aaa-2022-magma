{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from itertools import product\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from modules.config import *\n",
        "from modules.storage import get_results_df, get_availability_model_data\n",
        "from modules.neural_network import execute_stage, get_first_stage_hyperparameters,get_second_stage_hyperparameters, get_third_stage_hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Network Training\n",
        "In this notebook we train neural networks to predict availability. We first find the best subset of hyperparameters and then fit models with these hyperparameters in all resolutions.\n",
        "We use a multi-staged grid search, where we determine a set of best hyperparameters in one stage and then use it in the consecutive stages.\n",
        "While this approach will not guarantee to find the best hyperparameters, we expect it to be a good approximation, as a full scale grid search is computationally intractable.  \n",
        "\n",
        "To reduce training time, avoid overfitting and ensure that the model is sufficiently trained we use a high number of epochs and stop the model when the validation loss does not decrease any further.\n",
        "As we found that the validation loss changes in an unstable manner, we use a patience of 50 epochs.\n",
        "This means that Early Stopping will be activated after 50 epochs if the validation loss does not decrease any further.\n",
        "In addition we restore the weights of the epoch when the validation loss decreased last time.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_data(h3_res, time_interval_length):\n",
        "    model_data_train, model_data_test = get_availability_model_data(\n",
        "        h3_res, time_interval_length\n",
        "    )\n",
        "    return model_data_train, model_data_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the first stage, we find the best batch size by training simple single-layer Neural Networks, where the number of nodes is equal to the number of input features. The exact batch sizes, that we tried out, can be found in neural networks module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dacf253979314861beff8e9fc87caac0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13:26:37] batch_size: 512 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:37] batch_size: 256 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:37] batch_size: 128 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:37] batch_size: 64 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:37] batch_size: 32 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n"
          ]
        }
      ],
      "source": [
        "execute_stage(\n",
        "    get_model_data,\n",
        "    NN_FIRST_STAGE_AVAILABILITY_RESULTS_PATH,\n",
        "    get_first_stage_hyperparameters,\n",
        "    TUNE_H3_RESOLUTION,\n",
        "    TUNE_TIME_INTERVAL_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best batch_size: **64** - min: 32 - max: 512\n"
          ]
        }
      ],
      "source": [
        "results = get_results_df(NN_FIRST_STAGE_AVAILABILITY_RESULTS_PATH)\n",
        "\n",
        "best_batch_size = (\n",
        "    results[\n",
        "        (results[\"h3_res\"] == TUNE_H3_RESOLUTION)\n",
        "        & (results[\"time_interval_length\"] == TUNE_TIME_INTERVAL_LENGTH)\n",
        "    ]\n",
        "    .sort_values(by=\"val_mse\", ascending=True)[\"batch_size\"]\n",
        "    .iloc[0]\n",
        ")\n",
        "\n",
        "first_stage_hyperparameters = get_first_stage_hyperparameters()\n",
        "batch_sizes = list(map(lambda x: x['batch_size'], first_stage_hyperparameters))\n",
        "max_batch_size = max(batch_sizes)\n",
        "min_batch_size = min(batch_sizes)\n",
        "\n",
        "print(f\"best batch_size: **{best_batch_size}** - min: {min_batch_size} - max: {max_batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We find that the best batch size when predicting demand is **64**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the second stage, we find the best architecture of the model by varying the number of layers, nodes and two common hidden activation functions, namely the rectified linear unit and the hyperbolic tangent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66fe565e8da349dbb6ff6ac8f9c6da49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 1 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 2 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 2 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 3 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 0.5 - n_layers: 3 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 1 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 2 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 2 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 3 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1 - n_layers: 3 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 1 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 1 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 2 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 2 - activation: tanh - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: relu - dropout: -1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: -1 # already trained\n"
          ]
        }
      ],
      "source": [
        "get_hyperparameters = lambda : get_second_stage_hyperparameters(best_batch_size)\n",
        "execute_stage(\n",
        "    get_model_data,\n",
        "    NN_SECOND_STAGE_AVAILABILITY_RESULTS_PATH,\n",
        "    get_hyperparameters,\n",
        "    TUNE_H3_RESOLUTION,\n",
        "    TUNE_TIME_INTERVAL_LENGTH,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'nodes_per_feature': 1.5,\n",
              " 'n_layers': 3,\n",
              " 'activation': 'tanh'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = get_results_df(NN_SECOND_STAGE_AVAILABILITY_RESULTS_PATH)\n",
        "best_model = (\n",
        "    results[\n",
        "        (results[\"h3_res\"] == TUNE_H3_RESOLUTION)\n",
        "        & (results[\"time_interval_length\"] == TUNE_TIME_INTERVAL_LENGTH)\n",
        "    ]\n",
        "    .sort_values(by=\"val_mse\", ascending=True)\n",
        "    .iloc[0]\n",
        ")\n",
        "\n",
        "best_config = {\n",
        "    \"batch_size\": best_model[\"batch_size\"],\n",
        "    \"nodes_per_feature\": best_model[\"nodes_per_feature\"],\n",
        "    \"n_layers\": best_model[\"n_layers\"],\n",
        "    \"activation\": best_model[\"activation\"],\n",
        "}\n",
        "\n",
        "best_config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the third stage, we improve the generalizability by adding a dropout layer after every hidden layer and varying the dropout rate between 0 (no dropout) and 0.5 (dropout nodes half of the time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bff3a4e5a2d242949e5cb3fb44d89944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: 0 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: 0.05 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: 0.1 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: 0.2 # already trained\n",
            "[13:26:38] batch_size: 64 - nodes_per_feature: 1.5 - n_layers: 3 - activation: tanh - dropout: 0.5 # already trained\n"
          ]
        }
      ],
      "source": [
        "get_hyperparameters = lambda : get_third_stage_hyperparameters(\n",
        "    best_batch_size=best_model[\"batch_size\"],\n",
        "    best_nodes_per_feature=best_model[\"nodes_per_feature\"],\n",
        "    best_n_layers=best_model[\"n_layers\"],\n",
        "    best_activation=best_model[\"activation\"],\n",
        ")\n",
        "execute_stage(\n",
        "    get_model_data,\n",
        "    NN_THIRD_STAGE_AVAILABILITY_RESULTS_PATH,\n",
        "    get_hyperparameters,\n",
        "    TUNE_H3_RESOLUTION,\n",
        "    TUNE_TIME_INTERVAL_LENGTH,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'nodes_per_feature': 1.5,\n",
              " 'n_layers': 3,\n",
              " 'activation': 'tanh',\n",
              " 'dropout': 0.1}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = get_results_df(NN_THIRD_STAGE_AVAILABILITY_RESULTS_PATH)\n",
        "best_dropout = (\n",
        "\tresults[\n",
        "\t\t(results[\"h3_res\"] == TUNE_H3_RESOLUTION)\n",
        "\t\t& (results[\"time_interval_length\"] == TUNE_TIME_INTERVAL_LENGTH)\n",
        "\t]\n",
        "\t.sort_values(by=\"val_mse\", ascending=True)[\"dropout\"]\n",
        "\t.iloc[0]\n",
        ")\n",
        "best_config = {\n",
        "\t**best_config,\n",
        "\t\"dropout\": best_dropout,\n",
        "}\n",
        "best_config\n",
        "\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we will use these best hyperparameters to train one model for each H3 and time resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d36e92609234f2682288e25f3c2e5a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h3_res: 7, time_interval_length: 1 done\n",
            "h3_res: 7, time_interval_length: 2 done\n",
            "h3_res: 7, time_interval_length: 6 done\n",
            "h3_res: 7, time_interval_length: 24 done\n",
            "h3_res: 8, time_interval_length: 1 done\n",
            "h3_res: 8, time_interval_length: 2 done\n",
            "h3_res: 8, time_interval_length: 6 done\n",
            "h3_res: 8, time_interval_length: 24 done\n",
            "h3_res: 9, time_interval_length: 24 done\n"
          ]
        }
      ],
      "source": [
        "for h3_res, time_interval_length in tqdm(\n",
        "    list(product(PREDICTIVE_H3_RESOLUTIONS, CALC_TIME_INTERVAL_LENGTHS))\n",
        "    + ADDITIONAL_PREDICTIVE_RESOLUTIONS\n",
        "):\n",
        "    tqdm.write(f\"h3_res: {h3_res}, time_interval_length: {time_interval_length}\", end=\"\\r\")\n",
        "    execute_stage(\n",
        "        get_model_data,\n",
        "        NN_FOURTH_STAGE_AVAILABILITY_RESULTS_PATH,\n",
        "        lambda: [best_config],\n",
        "        h3_res,\n",
        "        time_interval_length,\n",
        "        test_phase=True,\n",
        "        silent=True,\n",
        "    )\n",
        "    tqdm.write(f\"h3_res: {h3_res}, time_interval_length: {time_interval_length} done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>h3_res</th>\n",
              "      <th>time_interval_length</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>nodes_per_feature</th>\n",
              "      <th>n_layers</th>\n",
              "      <th>activation</th>\n",
              "      <th>dropout</th>\n",
              "      <th>train_duration</th>\n",
              "      <th>test_mse</th>\n",
              "      <th>test_rmse</th>\n",
              "      <th>test_mae</th>\n",
              "      <th>test_non_zero_mape</th>\n",
              "      <th>test_zero_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>855.901519</td>\n",
              "      <td>45.103438</td>\n",
              "      <td>6.715909</td>\n",
              "      <td>2.734877</td>\n",
              "      <td>0.450544</td>\n",
              "      <td>0.855141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>560.716685</td>\n",
              "      <td>47.166135</td>\n",
              "      <td>6.867761</td>\n",
              "      <td>2.816211</td>\n",
              "      <td>0.442670</td>\n",
              "      <td>0.853768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>236.145673</td>\n",
              "      <td>53.590551</td>\n",
              "      <td>7.320557</td>\n",
              "      <td>3.066465</td>\n",
              "      <td>0.427757</td>\n",
              "      <td>0.846844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>24</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>94.846507</td>\n",
              "      <td>93.348040</td>\n",
              "      <td>9.661679</td>\n",
              "      <td>3.883434</td>\n",
              "      <td>0.341454</td>\n",
              "      <td>0.849619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3703.945640</td>\n",
              "      <td>5.674175</td>\n",
              "      <td>2.382053</td>\n",
              "      <td>0.983003</td>\n",
              "      <td>0.596280</td>\n",
              "      <td>0.865919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1830.123308</td>\n",
              "      <td>5.697243</td>\n",
              "      <td>2.386890</td>\n",
              "      <td>0.961181</td>\n",
              "      <td>0.589726</td>\n",
              "      <td>0.862052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>641.114363</td>\n",
              "      <td>6.773018</td>\n",
              "      <td>2.602502</td>\n",
              "      <td>1.063654</td>\n",
              "      <td>0.559425</td>\n",
              "      <td>0.853029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>230.024005</td>\n",
              "      <td>9.372776</td>\n",
              "      <td>3.061499</td>\n",
              "      <td>1.280754</td>\n",
              "      <td>0.477692</td>\n",
              "      <td>0.889425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>24</td>\n",
              "      <td>64</td>\n",
              "      <td>1.5</td>\n",
              "      <td>3</td>\n",
              "      <td>tanh</td>\n",
              "      <td>0.1</td>\n",
              "      <td>498.783041</td>\n",
              "      <td>1.812260</td>\n",
              "      <td>1.346202</td>\n",
              "      <td>0.646167</td>\n",
              "      <td>0.566795</td>\n",
              "      <td>0.822044</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   h3_res  time_interval_length  batch_size  nodes_per_feature  n_layers  \\\n",
              "0       7                     1          64                1.5         3   \n",
              "1       7                     2          64                1.5         3   \n",
              "2       7                     6          64                1.5         3   \n",
              "3       7                    24          64                1.5         3   \n",
              "4       8                     1          64                1.5         3   \n",
              "5       8                     2          64                1.5         3   \n",
              "6       8                     6          64                1.5         3   \n",
              "7       8                    24          64                1.5         3   \n",
              "8       9                    24          64                1.5         3   \n",
              "\n",
              "  activation  dropout  train_duration   test_mse  test_rmse  test_mae  \\\n",
              "0       tanh      0.1      855.901519  45.103438   6.715909  2.734877   \n",
              "1       tanh      0.1      560.716685  47.166135   6.867761  2.816211   \n",
              "2       tanh      0.1      236.145673  53.590551   7.320557  3.066465   \n",
              "3       tanh      0.1       94.846507  93.348040   9.661679  3.883434   \n",
              "4       tanh      0.1     3703.945640   5.674175   2.382053  0.983003   \n",
              "5       tanh      0.1     1830.123308   5.697243   2.386890  0.961181   \n",
              "6       tanh      0.1      641.114363   6.773018   2.602502  1.063654   \n",
              "7       tanh      0.1      230.024005   9.372776   3.061499  1.280754   \n",
              "8       tanh      0.1      498.783041   1.812260   1.346202  0.646167   \n",
              "\n",
              "   test_non_zero_mape  test_zero_accuracy  \n",
              "0            0.450544            0.855141  \n",
              "1            0.442670            0.853768  \n",
              "2            0.427757            0.846844  \n",
              "3            0.341454            0.849619  \n",
              "4            0.596280            0.865919  \n",
              "5            0.589726            0.862052  \n",
              "6            0.559425            0.853029  \n",
              "7            0.477692            0.889425  \n",
              "8            0.566795            0.822044  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = get_results_df(NN_FOURTH_STAGE_AVAILABILITY_RESULTS_PATH)\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('AAA_MAGMA_2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a7d5aa3afc48e507e79aab9da179a989cdb2272ea84b7e2d3626efebcf2c71f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
