{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Dev\\miniconda\\envs\\AAA_magma_2\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# for neural networks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# for evaluation & preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from modules.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_results_columns():\n",
    "    return [\n",
    "        \"h3_res\",\n",
    "        \"time_interval_length\",\n",
    "        \"batch_size\",\n",
    "        \"n_nodes\",\n",
    "        \"n_layers\",\n",
    "        \"activation\",\n",
    "        \"dropout\",\n",
    "        \"val_mse\", \"val_mae\", \"val_mape\", \"val_rmse\",\n",
    "        \"test_mse\", \"test_mae\", \"test_mape\", \"test_rmse\"\n",
    "    ]\n",
    "\n",
    "def get_results_df(path):\n",
    "    if os.path.isfile(path):\n",
    "        return pd.read_parquet(path)\n",
    "\n",
    "    results = pd.DataFrame(columns=get_nn_results_columns())\n",
    "    results.to_parquet(path)\n",
    "    return results\n",
    "\n",
    "\n",
    "def store_results(new_results, path):\n",
    "    results = pd.read_parquet(path)\n",
    "    results = pd.concat([results, new_results], ignore_index=True)\n",
    "    results.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method will get model data for a specific h3 resolution and time interval length\n",
    "def get_model_data(h3_res, time_interval_length):\n",
    "    model_data = pd.read_feather(os.path.join(MODEL_DATA_DIR_PATH, f\"{h3_res}_{time_interval_length}.feather\"))\n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale_data(model_data):\n",
    "    y = model_data[\"demand\"]\n",
    "    X = model_data.drop(columns=[\"demand\"])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.7, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, batch_size, n_nodes, n_layers, activation, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    for _ in range(n_layers):\n",
    "        model.add(Dense(n_nodes, activation=activation))\n",
    "        if dropout >= 0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"relu\"))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=batch_size, validation_split=0.25, callbacks=[early_stopping])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_average_percentage_error(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred) / y_true.mean()\n",
    "\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "def get_evaluation_metrics(y_true, y_pred, prefix):\n",
    "    return {\n",
    "        prefix+'_mse': mean_squared_error(y_true, y_pred),\n",
    "        prefix+'_mae': mean_absolute_error(y_true, y_pred),\n",
    "        prefix+'_mape': mean_average_percentage_error(y_true, y_pred),\n",
    "        prefix+'_rmse': root_mean_squared_error(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_meta_as_dict(model_meta):\n",
    "    return {\n",
    "        'batch_size': model_meta[0],\n",
    "        'n_nodes': model_meta[1],\n",
    "        'n_layers': model_meta[2],\n",
    "        'activation': model_meta[3],\n",
    "        'dropout': model_meta[4]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_first_stage_hyperparameters(n_features):\n",
    "    metas = {\n",
    "        'batch_size': [32, 64, 128, 256],\n",
    "        'n_nodes': [n_features],\n",
    "        'n_layers': [1],\n",
    "        'activation': ['relu'],\n",
    "        'dropout': [-1]\n",
    "    }\n",
    "    metas_list = list(product(*metas.values()))\n",
    "    models_metas = [get_model_meta_as_dict(model_meta) for model_meta in metas_list]\n",
    "    return models_metas\n",
    "\n",
    "\n",
    "def get_second_stage_hyperparameters(n_features, best_batch_size):\n",
    "    metas = {\n",
    "        'batch_size': [best_batch_size],\n",
    "        'n_nodes': [round(n_features*0.5), n_features, round(n_features*1.5)],\n",
    "        'n_layers': [1, 2, 3],\n",
    "        'activation': ['relu', 'sigmoid', 'tanh'],\n",
    "        'dropout': [-1]\n",
    "    }\n",
    "    metas_list = list(product(*metas.values()))\n",
    "    models_metas = [get_model_meta_as_dict(model_meta) for model_meta in metas_list]\n",
    "    return models_metas\n",
    "\n",
    "\n",
    "def get_third_stage_hyperparameters(best_batch_size, best_n_nodes, best_n_layers, best_activation):\n",
    "    metas = {\n",
    "        'batch_size': [best_batch_size],\n",
    "        'n_nodes': [best_n_nodes],\n",
    "        'n_layers': [best_n_layers],\n",
    "        'activation': [best_activation],\n",
    "        'dropout': [0, 0.05, 0.1, 0.2]\n",
    "    }\n",
    "    metas_list = list(product(*metas.values()))\n",
    "    models_metas = [get_model_meta_as_dict(model_meta) for model_meta in metas_list]\n",
    "    return models_metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_was_already_trained(results_path, h3_res, time_interval_length, model_params):\n",
    "    results = get_results_df(results_path)\n",
    "    return results[\n",
    "        (results['h3_res'] == h3_res) &\n",
    "        (results['time_interval_length'] == time_interval_length) &\n",
    "        (results['batch_size'] == model_params['batch_size']) &\n",
    "        (results['n_nodes'] == model_params['n_nodes']) &\n",
    "        (results['n_layers'] == model_params['n_layers']) &\n",
    "        (results['activation'] == model_params['activation']) &\n",
    "        (results['dropout'] == model_params['dropout'])\n",
    "    ]['val_mape'].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_stage(results_path, get_hyperparameters):\n",
    "    model_data = get_model_data(h3_res, time_interval_length)\n",
    "    model_data = model_data.iloc[:10000]\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = split_and_scale_data(model_data)\n",
    "    \n",
    "    for model_params in get_hyperparameters(X_train.shape[1]):\n",
    "        if not model_was_already_trained(results_path, h3_res, time_interval_length, model_params): continue\n",
    "\n",
    "        model = train_model(X_train, y_train, model_params['batch_size'], model_params['n_nodes'], model_params['n_layers'], model_params['activation'], model_params['dropout'])\n",
    "        y_pred_for_validation = model.predict(X_valid)\n",
    "        y_pred_for_test = model.predict(X_test)\n",
    "\n",
    "        results = {\n",
    "            'h3_res': h3_res,\n",
    "            'time_interval_length': time_interval_length,\n",
    "            'batch_size': model_params['batch_size'],\n",
    "            'n_nodes': model_params['n_nodes'],\n",
    "            'n_layers': model_params['n_layers'],\n",
    "            'activation': model_params['activation'],\n",
    "            'dropout': model_params['dropout'],\n",
    "\n",
    "            **get_evaluation_metrics(y_valid, y_pred_for_validation, 'val'),\n",
    "            **get_evaluation_metrics(y_test, y_pred_for_test, 'test'),\n",
    "        }\n",
    "        store_results(pd.DataFrame(data=results, index=[0]), results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h3_res in PREDICTIVE_H3_RESOLUTIONS:\n",
    "    for time_interval_length in CALC_TIME_INTERVAL_LENGTHS:\n",
    "        get_hyperparameters=lambda n_features: get_first_stage_hyperparameters(n_features)\n",
    "        execute_stage(NN_FIRST_STAGE_RESULTS_PATH, get_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results_df(NN_FIRST_STAGE_RESULTS_PATH)\n",
    "\n",
    "def get_best_batch_size(h3_res, time_interval_length):\n",
    "    return results[(results['h3_res'] == h3_res) & (results['time_interval_length'] == time_interval_length)].sort_values(by=\"val_mape\", ascending=True)['batch_size'].get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for h3_res in PREDICTIVE_H3_RESOLUTIONS:\n",
    "    for time_interval_length in CALC_TIME_INTERVAL_LENGTHS:\n",
    "        best_batch_size = get_best_batch_size(h3_res, time_interval_length)\n",
    "        print(best_batch_size)\n",
    "        # get_hyperparameters=lambda n_features: get_second_stage_hyperparameters(n_features, best_batch_size=best_batch_size)\n",
    "        # execute_stage(NN_SECOND_STAGE_RESULTS_PATH, get_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_results_df(NN_SECOND_STAGE_RESULTS_PATH)\n",
    "\n",
    "def get_best_model(h3_res, time_interval_length):\n",
    "    return results[(results['h3_res'] == h3_res) & (results['time_interval_length'] == time_interval_length)].sort_values(by=\"val_mape\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h3_res in PREDICTIVE_H3_RESOLUTIONS:\n",
    "    for time_interval_length in CALC_TIME_INTERVAL_LENGTHS:\n",
    "        best_model = get_best_model(h3_res, time_interval_length)\n",
    "        print(best_model)\n",
    "\n",
    "        get_hyperparameters=lambda n_features: get_third_stage_hyperparameters(\n",
    "            n_features,\n",
    "            best_batch_size=best_model['batch_size'].get(0),\n",
    "            best_n_nodes=best_model['n_nodes'].get(0),\n",
    "            best_n_layers=best_model['n_layers'].get(0),\n",
    "            best_activation=best_model['activation'].get(0)\n",
    "        )\n",
    "        execute_stage(NN_THIRD_STAGE_RESULTS_PATH, get_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3_res</th>\n",
       "      <th>time_interval_length</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>n_nodes</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>dropout</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mape</th>\n",
       "      <th>val_rmse</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_mape</th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.224808</td>\n",
       "      <td>0.304843</td>\n",
       "      <td>0.141568</td>\n",
       "      <td>0.474139</td>\n",
       "      <td>0.252091</td>\n",
       "      <td>0.323483</td>\n",
       "      <td>0.149047</td>\n",
       "      <td>0.502087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>-1</td>\n",
       "      <td>6.651429</td>\n",
       "      <td>2.153333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.579036</td>\n",
       "      <td>6.693000</td>\n",
       "      <td>2.170333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.587083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.534112</td>\n",
       "      <td>0.850916</td>\n",
       "      <td>0.395162</td>\n",
       "      <td>1.238593</td>\n",
       "      <td>1.544863</td>\n",
       "      <td>0.851981</td>\n",
       "      <td>0.392558</td>\n",
       "      <td>1.242925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>relu</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.050098</td>\n",
       "      <td>0.130238</td>\n",
       "      <td>0.060482</td>\n",
       "      <td>0.223826</td>\n",
       "      <td>0.064829</td>\n",
       "      <td>0.146753</td>\n",
       "      <td>0.067618</td>\n",
       "      <td>0.254616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>128.0</td>\n",
       "      <td>104</td>\n",
       "      <td>2</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.621680</td>\n",
       "      <td>0.946259</td>\n",
       "      <td>0.439439</td>\n",
       "      <td>1.273452</td>\n",
       "      <td>1.591798</td>\n",
       "      <td>0.942968</td>\n",
       "      <td>0.434481</td>\n",
       "      <td>1.261665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>556</td>\n",
       "      <td>3</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.340476</td>\n",
       "      <td>1.105238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.157789</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>1.117000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.189818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>556</td>\n",
       "      <td>3</td>\n",
       "      <td>tanh</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.113857</td>\n",
       "      <td>0.103016</td>\n",
       "      <td>0.147947</td>\n",
       "      <td>0.021298</td>\n",
       "      <td>0.111703</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.145940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>relu</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.029284</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>0.056057</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.028342</td>\n",
       "      <td>0.064805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.340476</td>\n",
       "      <td>1.105238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.157789</td>\n",
       "      <td>1.415667</td>\n",
       "      <td>1.117000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.189818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>834</td>\n",
       "      <td>1</td>\n",
       "      <td>tanh</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.202786</td>\n",
       "      <td>0.242065</td>\n",
       "      <td>0.219016</td>\n",
       "      <td>0.450318</td>\n",
       "      <td>0.229303</td>\n",
       "      <td>0.264949</td>\n",
       "      <td>0.237197</td>\n",
       "      <td>0.478856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     h3_res  time_interval_length  batch_size  n_nodes  n_layers activation  \\\n",
       "0         7                     1       128.0      104         1       relu   \n",
       "1         7                     1       128.0      104         1    sigmoid   \n",
       "2         7                     1       128.0      104         1       tanh   \n",
       "3         7                     1       128.0      104         2       relu   \n",
       "4         7                     1       128.0      104         2    sigmoid   \n",
       "..      ...                   ...         ...      ...       ...        ...   \n",
       "338       8                     1         NaN      556         3    sigmoid   \n",
       "339       8                     1         NaN      556         3       tanh   \n",
       "340       8                     1         NaN      834         1       relu   \n",
       "341       8                     1         NaN      834         1    sigmoid   \n",
       "342       8                     1         NaN      834         1       tanh   \n",
       "\n",
       "     dropout   val_mse   val_mae  val_mape  val_rmse  test_mse  test_mae  \\\n",
       "0         -1  0.224808  0.304843  0.141568  0.474139  0.252091  0.323483   \n",
       "1         -1  6.651429  2.153333  1.000000  2.579036  6.693000  2.170333   \n",
       "2         -1  1.534112  0.850916  0.395162  1.238593  1.544863  0.851981   \n",
       "3         -1  0.050098  0.130238  0.060482  0.223826  0.064829  0.146753   \n",
       "4         -1  1.621680  0.946259  0.439439  1.273452  1.591798  0.942968   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "338       -1  1.340476  1.105238  1.000000  1.157789  1.415667  1.117000   \n",
       "339       -1  0.021888  0.113857  0.103016  0.147947  0.021298  0.111703   \n",
       "340       -1  0.003142  0.029284  0.026496  0.056057  0.004200  0.031658   \n",
       "341       -1  1.340476  1.105238  1.000000  1.157789  1.415667  1.117000   \n",
       "342       -1  0.202786  0.242065  0.219016  0.450318  0.229303  0.264949   \n",
       "\n",
       "     test_mape  test_rmse  \n",
       "0     0.149047   0.502087  \n",
       "1     1.000000   2.587083  \n",
       "2     0.392558   1.242925  \n",
       "3     0.067618   0.254616  \n",
       "4     0.434481   1.261665  \n",
       "..         ...        ...  \n",
       "338   1.000000   1.189818  \n",
       "339   0.100003   0.145940  \n",
       "340   0.028342   0.064805  \n",
       "341   1.000000   1.189818  \n",
       "342   0.237197   0.478856  \n",
       "\n",
       "[343 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = get_results_df(NN_SECOND_STAGE_RESULTS_PATH)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('AAA_magma_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9d59ccc345e40e0b9baeaf74783fc4132100cce6ef264419ec664f37fe04324"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
